%\documentclass[handout]{beamer}
\documentclass[compress]{beamer}

\input{../resources/preamble}
\addbibresource{../resources/literature.bib}
\graphicspath{{../resources/pictures/}}

\title[Computational Communication Science 2]{\textbf{Computational Communication Science 2} \\Week 7 - Lecture\\ »Validation (in Supervised Machine Learning)«}
\author[Marthe Möller]{a.m.moller@uva.nl}
\date{May 15, 2023}
\institute[Digital Society Minor, University of Amsterdam]{Digital Society Minor, University of Amsterdam}


\begin{document}
	
	\begin{frame}{}
		\titlepage
	\end{frame}
	
	\begin{frame}{Today}
		\begin{tiny}
			\tableofcontents
		\end{tiny}
	\end{frame}


\section{Recap}

\begin{frame}[fragile]{Recap} 
	
\begin{alertblock}{Last week, we discussed:}
\begin{itemize}
	\item Supervised Machine Leaning (SML)
	\item The principles behind SML
	\item The steps of SML
	\item Some commonly used ML models
\end{itemize}
\end{alertblock}
	
\begin{alertblock}{At home, you:}
\begin{itemize}
	\item Got some hands-on experience SML (week 6 exercises)
\end{itemize}
\end{alertblock}	
\end{frame}


\begin{frame}[fragile]{Recap} 
\begin{alertblock}{Today, we:}
\begin{itemize}
	\item Review your first SML experience
	\item  Take a deep dive into validating SML-models
\end{itemize}
\end{alertblock}
	
\end{frame}


\begin{frame}[fragile]{Recap} 
	
\begin{alertblock}{Last week, you practiced with code that:}
\begin{itemize}
	\item Read in some data (Q1)
 	\item Split the data into a train and a test set (Q2)
	\item Set up a Count vectorizer (Q3)
	\item Trained a Naïve Bayes model with the count vectorizer (Q4)
	\item Requested some metrics for validation (Q5)
\end{itemize}
\end{alertblock}

\end{frame}


\begin{frame}[fragile]{Recap Q1}
	
\begin{lstlisting}
import csv
from collections import Counter
import matplotlib.pyplot as plt
			
file = "hatespeech_text_label_vote_RESTRICTED_100K.csv"
tweets = []
labels = []
		
with open(file) as fi:
  data = csv.reader(fi, delimiter='\t')
  for row in data:
   tweets.append(row[0])
   labels.append(row[1])
		
print(len(tweets) == len(labels)) # there should be just as many tweets as there are labels
		
Counter(labels)
plt.bar(Counter(labels).keys(), Counter(labels).values())
\end{lstlisting}

% you need two lists! 
\end{frame}



\begin{frame}[fragile]{Recap Q2}
Split the dataset:
\begin{lstlisting}
from sklearn.model_selection import train_test_split
		
tweets_train, tweets_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.2, random_state=42)
\end{lstlisting}
	
%What is the random_state?
%What is test_size?
\end{frame}


\begin{frame}[fragile]{Recap Q3}
What happens here?
\begin{lstlisting}
from sklearn.feature_extraction.text import (CountVectorizer)
		
countvectorizer = CountVectorizer(stop_words="english")
X_train = countvectorizer.fit_transform(tweets_train)
X_test = countvectorizer.transform(tweets_test)
\end{lstlisting}
%Remember from the first lectures? Same technique for a different goal!
\end{frame}



\begin{frame}[fragile]{Recap Q4}
The actual SML part (yes, truly, it is three lines of code!):
\begin{lstlisting}
nb = MultinomialNB()
nb.fit(X_train, y_train)
y_pred = nb.predict(X_test)
\end{lstlisting}
%No output?
\end{frame}

\begin{frame}[fragile]{Recap Q5}
You can check what was created:
\begin{lstlisting}
nb = MultinomialNB()
nb.fit(X_train, y_train)
y_pred = nb.predict(X_test)

print(y_pred[:10])
\end{lstlisting}
	
\begin{lstlistingoutput}
['normal' 'normal' 'normal' 'normal' 'spam' 'normal' 'normal' 'normal' 'abusive' 'normal']
\end{lstlistingoutput}
\end{frame}


\begin{frame}[fragile]{Recap Q5}
Classification report:
\begin{lstlisting}
from sklearn.metrics import classification_report
		
print(classification_report(y_test, y_pred))
\end{lstlisting}
	
%Learn from the documentation!
\end{frame}


\begin{frame}{Up next}
	
Classification report: validate your classifier. \\\
	
More about this today!
\end{frame}


\section{Validating models}

\begin{frame}{Validation}
Validation: When we assess the performance of a classifier. \\\
\pause
Or when we try to answer the question: "How well does the classifier work?"
\end{frame}

\begin{frame}{Validation}
What criteria should we use to decide on this?\\\
\pause
Entirely context specific!
\end{frame}


\begin{frame}[fragile]{Validation}
\begin{alertblock}{Compare different goals for using SML:}
	\begin{itemize}
		\item To automatically decide what Instagram users should see an advertisement
		\item To automatically remove spam from Twitter feed
	\end{itemize}
\end{alertblock}

Would you use the same criterion in both cases to determine how well a classifier works? Why (not)?
\end{frame}


\begin{frame}[fragile]{Validation}
There are various evaluation metrics available for machine learning. \\\
In scikit-learn, they are presented by ways of a classification report!
\end{frame}


\begin{frame}[fragile]{Zooming out} 
	
\begin{alertblock}{So far, we:}
\begin{itemize}
	\item Reviewed the exercise and the basic steps of SML
	\item Talked about what validation is
\end{itemize}
\end{alertblock}
	
\begin{alertblock}{Next, we will talk about:}
\begin{itemize}
	\item Some commonly used validation metrics
	\item Input for SML
	\item Finding the best classifier
\end{itemize}
\end{alertblock}	
\end{frame}

\section{Validation metrics}

\begin{frame}[fragile]{Precision}
Precision quantifies the number of positive class predictions that actually belong to the positive cases. \\\ 
\pause
OR: How much of what we found is actually correct?
\end{frame}

\begin{frame}{Precision}
Precision quantifies the number of positive class predictions that actually belong to the positive cases. \\\ 
OR: How much of what we found is actually correct? \\\


\begin{alertblock}{Compare different goals for using SML in exercise 6:}
\begin{itemize}
	\item To automatically decide what Instagram users should see an advertisement
	\item To automatically remove spam from Twitter feed
\end{itemize}
\end{alertblock}
\end{frame}



\begin{frame}{Recall}
Recall quantifies the number of positive class prediction made out of all positive examples in the dataset. \\\
\pause
OR: How many of the cases that we wanted to find did we actually find?
\end{frame}


\begin{frame}{Recall}
Recall quantifies the number of positive class prediction made out of all positive examples in the dataset. \\\
OR: How many of the cases that we wanted to find did we actually find?
	
\begin{alertblock}{Compare different goals for using SML in exercise 6:}
\begin{itemize}
	\item To automatically decide what Instagram users should see an advertisement
	\item To automatically remove spam from Twitter feed
\end{itemize}
\end{alertblock}
\end{frame}

\begin{frame}{Precision and Recall}
\begin{center}
	\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{../pictures/confusionmatrix_words.png} \\\
\end{center}
\end{frame}

\begin{frame}{Precision and Recall} 
	
\begin{columns}
	\column{.3\textwidth}
	\makebox[\columnwidth]{
		\includegraphics[width=\columnwidth,height=\paperheight,keepaspectratio]{../pictures/confusionmatrix_numbers.png}}
	\column{.7\textwidth}
	Precision is calculated as: \(\frac{\rm{TP}}{\rm{TP}+\rm{FP}}\) \\\
	In this example \(\frac{\rm{150}}{\rm{150}+\rm{50}}\) which is \(0.75\) \\\
	Recall is calculated as \(\frac{\rm{TP}}{\rm{TP}+\rm{FN}}\) \\\
	In this example \(\frac{\rm{150}}{\rm{150}+\rm{20}}\) which is \(0.88\)
\end{columns}
\end{frame}


\begin{frame}[fragile]{What does this look like in code?}
	
Let's ask for a confusion matrix: 
\begin{lstlisting}
from sklearn.metrics import confusion_matrix
		
y_test = [0, 1, 1, 1, 0]
y_pred = [0, 0, 1, 1, 1]
		
print(confusion_matrix(y_test, y_pred))
\end{lstlisting}
	
\begin{lstlistingoutput}
[[1  1 ]
[ 1  2]]
\end{lstlistingoutput}
\end{frame}

\begin{frame}[fragile]{The classification report}
	
Let's get some metrics for validation: 
	
\begin{lstlisting}
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
\end{lstlisting}

\begin{center}
\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{../pictures/classification-report} \\\
\end{center}

\end{frame}


\begin{frame}{\(F_1\)-score}
But wait...

\begin{alertblock}{Compare different goals for using SML:}
\begin{itemize}
	\item To automatically decide what Instagram users should see an advertisement
	\item To automatically remove spam from Twitter feed
\end{itemize}
\end{alertblock}

Such information was not available in the exercise for week 6!

%How do you know what is best - precision or recall?
\end{frame}


\begin{frame}{\(F_1\)-score}
\(F_1\)-score: The harmonic mean of precision and recall.
(Weighted average of precision and recall) \\
	
\(F_1\)-score \(= 2 \cdot \frac{\rm precision \cdot recall}{\rm precision + recall}\)
\end{frame}


\begin{frame}[fragile]{Accuracy}
	
\begin{center}
	\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{../pictures/classification-report} \\\
\end{center}
	
\end{frame}

\begin{frame}{Accuracy}
Accuracy: In which percentage of all cases was our classifier right? \\

% Rather general, always useful?
\end{frame}


\begin{frame}{Accuracy}
	
Class distribution: The number of examples that belong to each class. \\
	
Imbalanced classification: A predictive modeling problem where the distribution of examples across the classes within a training dataset is not equal.
	
% Not useful if you are dealing with class imbalance!
\end{frame}


\begin{frame}[fragile]{Accuracy}
\begin{center}
	\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{../pictures/barplot} \\\
\end{center}
\end{frame}

\begin{frame}{Accuracy}
	
\begin{center}
	\includegraphics[width=\linewidth,height=\textheight,keepaspectratio]{../pictures/imbalance.png} 
\end{center}	

Majority class (red dots) vs. minority class (grey dots) 
\end{frame}

\begin{frame}{Accuracy}
Always check how your cases are distributed across the labels.
\end{frame}


\begin{frame}{Validation: Best practices}
Decide what metric is best to use beforehand.

\begin{alertblock}{Consider:}
	\begin{itemize}
		\item Is class imbalance an issue? 
		\item What will the classifier be used for?
	\end{itemize}
\end{alertblock}

The latter can bring you back to the question: To SML or not to SML?
\end{frame}

\begin{frame}{Validation: Best practices}

\begin{alertblock}{SML suitability depends on:}
\begin{itemize}
	\item How hard/easy it is to translate the decision proces for classification into straight-forward rules
	\item How much data there are to classify
	\item How much room there is for errors 
\end{itemize}
\end{alertblock}
Consider these matters as you decide (a) whether or not to use SML, and (b) what performance metrics to use.
\end{frame}


\begin{frame}{Validation: Best practices}
In last week's lecture, we saw that you can train many different classifiers.  

\begin{alertblock}{Amongst other, classifiers can differ based on:}
	\begin{itemize}
		\item The vectorizer that is used on the data (i.e., count vectorizer or tf-idf vectorizer)
		\item The underlying model (e.g., Naïve Bayes, Logistic Regression, Decision Trees, SVM, etc.)\\\
	\end{itemize}
\end{alertblock}
\pause
How do you know what classifier is best beforehand?
\end{frame}


\begin{frame}{Selecting a classifier}

You don't!\\\
\pause
Typically, various classifiers are trained and their performance is compared.\\\
\pause
The best performing classifier is then selected and used to annotate more/new data.
%iterative process
\end{frame}



\begin{frame}[fragile]{Zooming out} 
	
\begin{alertblock}{So far, we:}
\begin{itemize}
	\item Reviewed the exercise and the basic steps of SML
	\item Talked about what validation is
	\item Discussed some commonly used validation metrics
\end{itemize}
\end{alertblock}
	
\begin{alertblock}{Next, we will talk about:}
\begin{itemize}
	\item Input for SML
\end{itemize}
\end{alertblock}	
\end{frame}



\section{About input for SML}

\begin{frame}[fragile]{Bad input}
\begin{alertblock}{Poor quality input can cause:}
	\begin{itemize}
		\item Your classifier not being able to identify specific categories %class imbalance
		\item Your classifier to perform badly overall %hand coding unreliable
		\item Your classifer to pick up implicit bias %bias in the hand coding
	\end{itemize}
\end{alertblock}
\end{frame}


\begin{frame}[fragile]{Bad input}
What makes input (training data) bad input?
\end{frame}


\begin{frame}[fragile]{Bad input}
What can be done to solve these issues?
\begin{alertblock}{Poor quality input can cause:}
	\begin{itemize}
		\item Your classifier not being able to identify specific categories %class imbalance
		\item Your classifier to perform badly overall %hand coding unreliable
		\item Your classifer to pick up implicit bias %bias in the hand coding
	\end{itemize}
\end{alertblock}
\end{frame}



\begin{frame}[fragile]{Creating good input}
\begin{alertblock}{You can create good input:}
	\begin{itemize}
		\item Avoid class imbalance: increase your sample (add rare cases), or random selection %class imbalance
		\item Make sure the hand coding is reliable (check Krippendorf's Alpha, add coder training) %hand coding unreliable
		\item Make sure to develop a clear and objective codebook %bias in the hand coding
	\end{itemize}
\end{alertblock}
\end{frame}


\begin{frame}[fragile]{The limits of your input}

User case*: \\\
A classifier was trained to distinguish between YouTube comments that address the video that they accompany, and YouTube comments that do not.
\pause
The classifier was trained on a training set consisting of > 16.000 comments written in response to music videos.
\pause
The classifier performed well, with high scores on all evaluation metrics.
\pause
The researchers now want to use their classifier to automatically label a new set of 10.000 YouTube comments written in response to vlogs.
\pause
Can they? \\\

\begin{tiny}
*Based on: Möller et al., in press
\end{tiny}
\end{frame}

\begin{frame}[fragile]{The limits of your input}
Classifiers can only be used to automatically annotate data from the same population as the data that they were trained on.
\end{frame}


\begin{frame}[fragile]{Zooming out} 
	
\begin{alertblock}{Today, we:}
\begin{itemize}
	\item Reviewed the exercise and the basic steps of SML
	\item Talked about what validation is
	\item Discussed some commonly used validation metrics
	\item Talked about input for SML
\end{itemize}
\end{alertblock}
	
\end{frame}


\begin{frame}[fragile]{Zooming out} 
	
\begin{alertblock}{Tomorrow and this week, you will:}
\begin{itemize}
	\item Set up multiple different classifiers
	\item Validate those classifiers
	\item Select the best performing classifier
\end{itemize}
\end{alertblock}
	
Work on the tutorial exercises for this week. \\\
\end{frame}


\end{document}